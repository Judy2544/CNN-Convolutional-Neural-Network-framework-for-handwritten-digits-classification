{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Judy2544/CNN-for-handwritten-digits-classification/blob/main/BDM_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3\n",
        "\n",
        "We are greatly inspired by the [Consumer Complaints](https://github.com/InsightDataScience/consumer_complaints) challenge from [InsightDataScience](https://github.com/InsightDataScience/). In fact, we are going to tackle the same challenge but using Apache Spark. Please read through the challenge at the following link:\n",
        "\n",
        "<https://github.com/InsightDataScience/consumer_complaints>\n",
        "\n",
        "The most important sections are **Input dataset** and **Expected output**, which are quoted below:\n",
        "\n",
        "## Input dataset\n",
        "For this challenge, when we grade your submission, an input file, `complaints.csv`, will be moved to the top-most `input` directory of your repository. Your code must read that input file, process it and write the results to an output file, `report.csv` that your code must place in the top-most `output` directory of your repository.\n",
        "\n",
        "Below are the contents of an example `complaints.csv` file: \n",
        "```\n",
        "Date received,Product,Sub-product,Issue,Sub-issue,Consumer complaint narrative,Company public response,Company,State,ZIP code,Tags,Consumer consent provided?,Submitted via,Date sent to company,Company response to consumer,Timely response?,Consumer disputed?,Complaint ID\n",
        "2019-09-24,Debt collection,I do not know,Attempts to collect debt not owed,Debt is not yours,\"transworld systems inc. is trying to collect a debt that is not mine, not owed and is inaccurate.\",,TRANSWORLD SYSTEMS INC,FL,335XX,,Consent provided,Web,2019-09-24,Closed with explanation,Yes,N/A,3384392\n",
        "2019-09-19,\"Credit reporting, credit repair services, or other personal consumer reports\",Credit reporting,Incorrect information on your report,Information belongs to someone else,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,Experian Information Solutions Inc.,PA,15206,,Consent not provided,Web,2019-09-20,Closed with non-monetary relief,Yes,N/A,3379500\n",
        "2020-01-06,\"Credit reporting, credit repair services, or other personal consumer reports\",Credit reporting,Incorrect information on your report,Information belongs to someone else,,,Experian Information Solutions Inc.,CA,92532,,N/A,Email,2020-01-06,In progress,Yes,N/A,3486776\n",
        "2019-10-24,\"Credit reporting, credit repair services, or other personal consumer reports\",Credit reporting,Incorrect information on your report,Information belongs to someone else,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,\"TRANSUNION INTERMEDIATE HOLDINGS, INC.\",CA,925XX,,Other,Web,2019-10-24,Closed with explanation,Yes,N/A,3416481\n",
        "2019-11-20,\"Credit reporting, credit repair services, or other personal consumer reports\",Credit reporting,Incorrect information on your report,Account information incorrect,I would like the credit bureau to correct my XXXX XXXX XXXX XXXX balance. My correct balance is XXXX,Company has responded to the consumer and the CFPB and chooses not to provide a public response,\"TRANSUNION INTERMEDIATE HOLDINGS, INC.\",TX,77004,,Consent provided,Web,2019-11-20,Closed with explanation,Yes,N/A,3444592\n",
        "```\n",
        "Each line of the input file, except for the first-line header, represents one complaint. Consult the [Consumer Finance Protection Bureau's technical documentation](https://cfpb.github.io/api/ccdb/fields.html) for a description of each field.  \n",
        "\n",
        "* Notice that complaints were not listed in chronological order\n",
        "* In 2019, there was a complaint against `TRANSWORLD SYSTEMS INC` for `Debt collection` \n",
        "* Also in 2019, `Experian Information Solutions Inc.` received one complaint for `Credit reporting, credit repair services, or other personal consumer reports` while `TRANSUNION INTERMEDIATE HOLDINGS, INC.` received two\n",
        "* In 2020, `Experian Information Solutions Inc.` received a complaint for `Credit reporting, credit repair services, or other personal consumer reports`\n",
        "\n",
        "In summary that means \n",
        "* In 2019, there was one complaint for `Debt collection`, and 100% of it went to one company \n",
        "* Also in 2019, three complaints against two companies were received for `Credit reporting, credit repair services, or other personal consumer reports` and 2/3rd of them (or 67% if we rounded the percentage to the nearest whole number) were against one company (TRANSUNION INTERMEDIATE HOLDINGS, INC.)\n",
        "* In 2020, only one complaint was received for `Credit reporting, credit repair services, or other personal consumer reports`, and so the highest percentage received by one company would be 100%\n",
        "\n",
        "For this challenge, we want for each product and year that complaints were received, the total number of complaints, number of companies receiving a complaint and the highest percentage of complaints directed at a single company.\n",
        "\n",
        "For the purposes of this challenge, all names, including company and product, should be treated as case insensitive. For example, \"Acme\", \"ACME\", and \"acme\" would represent the same company.\n",
        "\n",
        "## Expected output\n",
        "\n",
        "After reading and processing the input file, your code should create an output file, `report.csv`, with as many lines as unique pairs of product and year (of `Date received`) in the input file. \n",
        "\n",
        "Each line in the output file should list the following fields in the following order:\n",
        "* product (name should be written in all lowercase)\n",
        "* year\n",
        "* total number of complaints received for that product and year\n",
        "* total number of companies receiving at least one complaint for that product and year\n",
        "* highest percentage (rounded to the nearest whole number) of total complaints filed against one company for that product and year. Use standard rounding conventions (i.e., Any percentage between 0.5% and 1%, inclusive, should round to 1% and anything less than 0.5% should round to 0%)\n",
        "\n",
        "The lines in the output file should be sorted by product (alphabetically) and year (ascending)\n",
        "\n",
        "Given the above `complaints.csv` input file, we'd expect an output file, `report.csv`, in the following format\n",
        "```\n",
        "\"credit reporting, credit repair services, or other personal consumer reports\",2019,3,2,67\n",
        "\"credit reporting, credit repair services, or other personal consumer reports\",2020,1,1,100\n",
        "debt collection,2019,1,1,100\n",
        "```\n",
        "Notice that because `debt collection` was only listed for 2019 and not 2020, the output file only has a single entry for debt collection. Also, notice that when a product has a comma (`,`) in the name, the name should be enclosed by double quotation marks (`\"`). Finally, notice that percentages are listed as numbers and do not have `%` in them.\n",
        "\n",
        "# Objectives\n",
        "\n",
        "In this homework, we will tackle the above problem in two steps (2 tasks):\n",
        "\n",
        "1. In Task 1, we work on a solution with PySpark on Google Colab using a sample of the data. The data is available on Google Drive and is to be downloaded by the `gdown` command in Task 1.\n",
        "\n",
        "2. In Task 2, we create a standalone Python script that work on the full dataset using NYU HPC DataProc cluster. The full dataset is downloaded from [here](https://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data). The data is available on NYU cluster HDFS at: `/shared/CUSP-GX-6002/data/complaints.csv`\n",
        "\n"
      ],
      "metadata": {
        "id": "gbF71GHptuwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "VBWF5LNefozN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjzjcPWYnHLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635bd315-3598-41a7-c04f-8658bc1a7f1e"
      },
      "source": [
        "%%shell\n",
        "gdown --quiet 1-IeoZDwT5wQzBUpsaS5B6vTaP-2ZBkam\n",
        "pip --quiet install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHKq10WXnZl7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "b3c4931d-774f-49fa-b077-8aede09a70bd"
      },
      "source": [
        "COMPLAINTS_FN = 'complaints_sample.csv'\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f76db1bb250>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://fa9373dff537:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Ykp1Qqnu5f"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "Use PySpark to derive the expected output. Your computation must be done entirely on Spark's transformation. The output MUST be in the CSV form, i.e. each output line is a complete comma separated string that can be fed into a CSV reader. It is okay if your output are divided into multiple parts (due to the nature of distributed computing of Spark)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputTask1 is an output RDD, you can use DataFrame as well but each line\n",
        "# still needs to be a string\n",
        "outputTask1.take(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1r07D6MEMOq",
        "outputId": "2975339e-40e2-4354-de9d-5e46d530c92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bank account or service,2015,1,1,100',\n",
              " 'bank account or service,2016,1,1,100',\n",
              " 'checking or savings account,2017,1,1,100',\n",
              " 'checking or savings account,2018,15,8,27',\n",
              " 'checking or savings account,2019,422,72,14',\n",
              " 'checking or savings account,2020,3,3,33',\n",
              " 'consumer loan,2015,1,1,100',\n",
              " 'consumer loan,2017,1,1,100',\n",
              " 'credit card,2016,3,3,33',\n",
              " 'credit card or prepaid card,2017,1,1,100',\n",
              " 'credit card or prepaid card,2018,19,9,26',\n",
              " 'credit card or prepaid card,2019,382,39,16',\n",
              " 'credit card or prepaid card,2020,13,10,23',\n",
              " 'credit reporting, credit repair services, or other personal consumer reports,2017,5,3,40',\n",
              " 'credit reporting, credit repair services, or other personal consumer reports,2018,185,17,58',\n",
              " 'credit reporting, credit repair services, or other personal consumer reports,2019,2899,196,50',\n",
              " 'credit reporting, credit repair services, or other personal consumer reports,2020,137,9,52',\n",
              " 'debt collection,2015,4,3,50',\n",
              " 'debt collection,2016,9,4,67',\n",
              " 'debt collection,2017,10,9,20']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2\n",
        "\n",
        "For this task, please convert what you have in Task 1 to a standalone file that can be run on the cluster. The input and output locations must be taken from the command line, e.g.:\n",
        "\n",
        "```shell\n",
        "spark-submit BDM_HW3_NetID.py /shared/CUSP-GX-6002/data/complaints.csv /shared/CUSP-GX-6002/HW3/NetID\n",
        "```\n",
        "\n",
        "As part of the test, you must be able to run your code and output to the class shared folder, i.e.: `/shared/CUSP-GX-6002/HW3/NetID`, **replacing `NetID` with your actual NetID**.\n",
        "\n",
        "Note that, if you run your code multiple times, make sure to only run your working version when output to the shared folder, or you must remove the existing output to run your code again."
      ],
      "metadata": {
        "id": "oNXJMMLEEcHB"
      }
    }
  ]
}